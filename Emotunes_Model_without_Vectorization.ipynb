{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wmnOVE4W2nXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6d38be-85eb-4da9-e5c4-e45e6a34a4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 24s 46ms/step - loss: 0.7116 - accuracy: 0.7405 - val_loss: 0.1930 - val_accuracy: 0.9225\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 21s 41ms/step - loss: 0.1248 - accuracy: 0.9477 - val_loss: 0.1945 - val_accuracy: 0.9175\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 21s 42ms/step - loss: 0.0561 - accuracy: 0.9788 - val_loss: 0.2070 - val_accuracy: 0.9205\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 22s 44ms/step - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.2210 - val_accuracy: 0.9235\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 23s 46ms/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 0.2600 - val_accuracy: 0.9215\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 19s 38ms/step - loss: 0.0146 - accuracy: 0.9958 - val_loss: 0.2821 - val_accuracy: 0.9185\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 24s 48ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 0.2887 - val_accuracy: 0.9220\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 20s 40ms/step - loss: 0.0125 - accuracy: 0.9968 - val_loss: 0.2914 - val_accuracy: 0.9175\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 24s 49ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.3486 - val_accuracy: 0.9105\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 23s 46ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.3390 - val_accuracy: 0.9155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b45324b76a0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import layers, models\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# NLTK Downloads\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocess the data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(sentence):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Load data\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=['sentence', 'label'], sep=';')\n",
        "    data['sentence'] = data['sentence'].apply(preprocess_text)\n",
        "    return data\n",
        "\n",
        "train_data = load_data('/content/cleaned_train.txt')\n",
        "val_data = load_data('/content/val.txt')\n",
        "test_data = load_data('/content/test.txt')\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
        "val_data['label'] = label_encoder.transform(val_data['label'])\n",
        "test_data['label'] = label_encoder.transform(test_data['label'])\n",
        "\n",
        "# Tokenization and Vectorization\n",
        "max_tokens = 12000\n",
        "output_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_tokens, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_data['sentence'])\n",
        "\n",
        "train_sequences = get_sequences(tokenizer, train_data['sentence'])\n",
        "val_sequences = get_sequences(tokenizer, val_data['sentence'])\n",
        "test_sequences = get_sequences(tokenizer, test_data['sentence'])\n",
        "\n",
        "# Model setup\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=max_tokens + 1, output_dim=128, input_length=output_sequence_length),\n",
        "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(128, activation='relu'),  # Additional ANN layer\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_sequences, train_data['label'], epochs=10, validation_data=(val_sequences, val_data['label']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING AND TESTING THE TfLITE MODEL WITH NEW TEXT INPUT"
      ],
      "metadata": {
        "id": "m5oBQP0kotFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "import json\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/modelV2.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "# Load your saved tokenizer\n",
        "with open('tokenizerV2.json') as f:\n",
        "    data = json.load(f)\n",
        "    tokenizer = tokenizer_from_json(data)\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_input_text(text, tokenizer, max_length):\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "yG3IkHqkku5m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text input\n",
        "input_text = \"Today I will be presenting my research results, Im so nervous that Im feeling nauseous\"\n",
        "\n",
        "max_length = 100\n",
        "preprocessed_text = preprocess_input_text(input_text, tokenizer, max_length)\n",
        "\n",
        "# Set the tensor to point to the input data to be inferred\n",
        "interpreter.set_tensor(input_details[0]['index'], np.float32(preprocessed_text))\n",
        "\n",
        "# Run the inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Extract the output\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Assuming the output is a probability distribution over labels\n",
        "predicted_label_index = np.argmax(output_data, axis=1)\n",
        "\n",
        "# Convert predicted_label_index to the corresponding label\n",
        "predicted_label = label_encoder.inverse_transform(predicted_label_index)\n",
        "print(f\"Predicted Emotion: {predicted_label[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H7ju6tjlrcZ",
        "outputId": "d1bc5ace-e792-46e0-e904-d86dc1a7c886"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: fear\n"
          ]
        }
      ]
    }
  ]
}